import streamlit as st
import fitz  # PyMuPDF
from docx import Document
import google.generativeai as genai

# --------------------------
# üîë Configure Gemini API Key
# --------------------------
# Recommended: put your key in Streamlit secrets (see note below)
genai.configure(api_key="AIzaSyAXWljeX6aYPAINcW9QasYVkI_9i67O79k")
model = genai.GenerativeModel("gemini-1.5-flash")  # you can also try "gemini-pro"

# --------------------------
# üìÑ File Parsing Functions
# --------------------------
def extract_text_from_pdf(file):
    """Extracts text from a PDF file."""
    doc = fitz.open(stream=file.read(), filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def extract_text_from_docx(file):
    """Extracts text from a DOCX file."""
    doc = Document(file)
    text = []
    for para in doc.paragraphs:
        text.append(para.text)
    return "\n".join(text)

# --------------------------
# ü§ñ LLM Analysis Function
# --------------------------
def analyze_document_with_llm(text, task):
    """Sends text to Gemini for analysis."""
    prompt = f"{task}\n\nDocument Text:\n{text[:4000]}"  # limit to first 4000 chars
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"An error occurred: {e}"

# --------------------------
# üé® Streamlit UI
# --------------------------
st.set_page_config(page_title="Contract Analyzer", layout="wide")
st.title("üìÑ Contract Analyzer")

# File Upload
st.header("Upload a Contract")
uploaded_file = st.file_uploader("Choose a PDF or DOCX file", type=["pdf", "docx"])

if uploaded_file:
    with st.spinner("Processing document..."):
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        if file_extension == 'pdf':
            full_text = extract_text_from_pdf(uploaded_file)
        elif file_extension == 'docx':
            full_text = extract_text_from_docx(uploaded_file)
        else:
            st.error("Unsupported file type.")
            full_text = None

    if full_text:
        st.success("‚úÖ Document uploaded and text extracted!")

        # --- Analysis Sections ---
        st.markdown("---")
        st.header("Analysis Results")
        
        # Summary
        with st.expander("üìå Summary", expanded=True):
            with st.spinner("Generating summary..."):
                summary_prompt = "Provide a concise summary of the key points in the following contract."
                summary = analyze_document_with_llm(full_text, summary_prompt)
                st.write(summary)

        # Important Clauses
        with st.expander("‚öñÔ∏è Important Clauses", expanded=True):
            with st.spinner("Extracting important clauses..."):
                clauses_prompt = "Identify and list the most important clauses from the following contract, such as termination, liability, and payment terms."
                clauses = analyze_document_with_llm(full_text, clauses_prompt)
                st.write(clauses)

        st.markdown("---")
        
        # Q&A Chat
        st.header("üí¨ Chat with the Document")
        if "messages" not in st.session_state:
            st.session_state.messages = []

        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        if user_prompt := st.chat_input("Ask a question about the document..."):
            st.session_state.messages.append({"role": "user", "content": user_prompt})
            with st.chat_message("user"):
                st.markdown(user_prompt)

            with st.chat_message("assistant"):
                with st.spinner("Thinking..."):
                    chat_prompt = f"Based on the following document, answer this question: {user_prompt}\n\nDocument Text:\n{full_text[:4000]}"
                    response = analyze_document_with_llm(full_text, chat_prompt)
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})


